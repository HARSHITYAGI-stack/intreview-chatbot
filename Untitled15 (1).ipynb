# @title Setup and Run Interview Bot (Run this cell)
# Install necessary libraries
!pip install openai ipywidgets -q

import openai
from openai import OpenAI
import ipywidgets as widgets
from IPython.display import display, clear_output, Javascript
import time
import os
import random

def display_summary():
    """Displays the final interview summary."""
    clear_output(wait=True)  # Clear everything including initial setup widgets
    print("=" * 40)
    print("    ✅ Interview Completed! ✅")
    print("=" * 40 + "\n")
    print(f"Role: {interview_state['role']}, Mode: {interview_state['mode']}\n")
    print("--- Interview Summary ---")

    total_score = 0
    scored_questions = 0

    if not interview_state['results']:
        print("\nNo questions were answered.")
    else:
        for i, result in enumerate(interview_state['results']):
            print(f"\n--- Question {i + 1} ---")
            print(f"Q: {result['question']}")
            # Truncate long answers for display if needed
            answer_display = result['answer']
            if len(answer_display) > 300:
                answer_display = answer_display[:300] + "..."
            print(f"A: {answer_display}")
            print(f"\nFeedback:\n{result['feedback']}")
            if result['score'] is not None:
                print(f"Score: {result['score']}/10")
                total_score += result['score']
                scored_questions += 1
            else:
                print("Score: N/A")

    print("\n" + "=" * 25)
    if scored_questions > 0:
        average_score = total_score / scored_questions
        print(f"Overall Average Score: {average_score:.1f} / 10 ({scored_questions} scored questions)")
    elif client:
        print("No scorable answers were evaluated.")
    else:
        print("Evaluation was disabled (OpenAI client not initialized).")
    print("--- End of Summary ---")

    # Suggestion to restart
    print("\nRun the first cell again to start a new interview.")

# --- Configuration ---
# IMPORTANT: Replace with your actual OpenAI API key
# It's best practice to use Colab Secrets or environment variables
# from google.colab import userdata
# try:
#     OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
#     if not OPENAI_API_KEY:
#         raise ValueError("OpenAI API key not found in Colab Secrets. Please add it.")
# except ImportError:
#     # Fallback if not in Colab or userdata is not used
#     OPENAI_API_KEY = "YOUR_OPENAI_API_KEY" # <-- PASTE YOUR KEY HERE if not using Secrets
#     if OPENAI_API_KEY == "YOUR_OPENAI_API_KEY":
#        print("⚠️ WARNING: Replace 'YOUR_OPENAI_API_KEY' with your actual key.")

# --- Paste your API Key directly here for simplicity in this example ---
# --- (Remember this is less secure than using Colab Secrets) ---
#OPENAI_API_KEY = "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" # <--- *** PASTE YOUR KEY HERE ***
OPENAI_API_KEY = "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"
#OPENAI_API_KEY = "sk-...oZ4A"
# --- OpenAI Client Initialization ---
client = None
if OPENAI_API_KEY and OPENAI_API_KEY != "OPENAI_API_KEY":
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        # Test the client connection (optional but recommended)
        client.models.list()
        print("✅ OpenAI client initialized successfully.")
    except openai.AuthenticationError:
        print("❌ Authentication Error: Invalid OpenAI API key provided.")
        print("   Please check your API key and restart the runtime.")
        client = None
    except Exception as e:
        print(f"❌ An unexpected error occurred during OpenAI client initialization: {e}")
        client = None
else:
    print("❌ OpenAI API key not provided. Evaluation functionality will be disabled.")
    print("   Please add your key to the OPENAI_API_KEY variable and restart the runtime.")
    client = None

# --- Question Generation (Example Hardcoded - Can be LLM-generated) ---
def generate_questions(role, domain, mode, num_questions=4):
    """Generates a list of interview questions."""
    all_questions = {
        "Software Engineer": {
            "Frontend": {
                "Technical": [
                    "Explain the concept of the Virtual DOM and how it improves performance in frameworks like React.",
                    "What are CSS preprocessors (like Sass or Less) and what benefits do they offer?",
                    "Describe the difference between `localStorage`, `sessionStorage`, and `cookies`.",
                    "How would you approach optimizing the loading performance of a large single-page application?",
                    "Explain the CSS box model.",
                    "What is CORS and why is it important?",
                ],
                "Behavioral": [
                    "Describe a time you had a disagreement with a designer about a UI/UX decision. How did you resolve it?",
                    "Tell me about a challenging frontend bug you fixed. What was the process?",
                    "How do you stay updated with the latest frontend technologies and trends?",
                ]
            },
            "Backend": {
                "Technical": [
                    "Explain the difference between SQL and NoSQL databases. Give examples of use cases for each.",
                    "What is RESTful API design? Describe the key principles.",
                    "How does database indexing work and why is it important for performance?",
                    "Describe the request/response cycle in a typical web application.",
                    "What are microservices and what are their pros and cons compared to monolithic architectures?",
                    "Explain the concept of caching and different caching strategies (e.g., client-side, server-side, CDN).",
                ],
                 "Behavioral": [
                    "Tell me about a time you had to design a system to handle a significant increase in load.",
                    "Describe a situation where you identified and fixed a security vulnerability in a backend system.",
                     "How do you approach debugging complex issues in a distributed backend system?",
                 ]
            },
            "ML": {
                 "Technical": [
                    "Explain the bias-variance tradeoff in machine learning.",
                    "What is overfitting, and what are common techniques to prevent it?",
                    "Describe the difference between supervised, unsupervised, and reinforcement learning.",
                    "How would you evaluate the performance of a classification model?",
                    "Explain gradient descent and its role in training neural networks.",
                 ],
                 "Behavioral": [
                    "Describe a time you had to explain a complex ML concept to a non-technical stakeholder.",
                    "Tell me about an ML project where the results were not what you expected. How did you handle it?",
                    "How do you ensure fairness and mitigate bias in machine learning models?",
                 ]
            },
            "System Design": {
                 "Technical": [
                    "How would you design a URL shortening service like TinyURL?",
                    "Design a system like Twitter's feed. What are the key components and considerations?",
                    "Explain the concept of load balancing and different algorithms.",
                    "What is sharding and how does it help scale databases?",
                    "Describe the CAP theorem and its implications in distributed systems.",
                 ],
                 "Behavioral": [
                     "Walk me through the process of designing a system when requirements are initially vague.",
                     "Describe a time you had to make a significant architectural decision with long-term consequences.",
                     "How do you handle disagreements about system design choices within a team?",
                 ]
            },
            "General": { # Fallback if domain is None
                 "Technical": [
                    "Describe a challenging technical problem you solved recently. What was your approach?",
                    "Explain the difference between processes and threads.",
                    "What is Object-Oriented Programming? Explain its main principles.",
                    "Talk about a version control system you've used (like Git) and explain basic commands.",
                 ],
                 "Behavioral": [ # Generic behavioral questions
                    "Tell me about yourself and why you're interested in this role.",
                    "Describe a time you failed. What did you learn from it?",
                    "How do you handle working under pressure or tight deadlines?",
                    "Where do you see yourself in 5 years?",
                    "Why should we hire you?",
                 ]
            }
        },
        "Product Manager": {
            "General": { # PM roles often less domain-specific in interviews
                "Technical": [ # Less common, but possible
                    "How do you prioritize features when you have limited engineering resources?",
                    "Explain the concept of A/B testing and how you would use it.",
                    "What metrics would you track to measure the success of a new product launch?",
                 ],
                "Behavioral": [
                    "How do you handle conflicting priorities from different stakeholders?",
                    "Describe a product you admire and why. What would you improve?",
                    "Tell me about a time you had to say 'no' to a feature request.",
                    "How do you work with engineering teams? Describe your ideal collaboration process.",
                    "Walk me through how you would define the MVP for a new product idea.",
                    "How do you gather and incorporate user feedback into the product roadmap?",
                 ]
            }
        },
        "Data Analyst": {
             "General": {
                 "Technical": [
                    "Explain the difference between primary keys and foreign keys in SQL.",
                    "Describe different types of SQL joins (INNER, LEFT, RIGHT, FULL OUTER).",
                    "How would you clean a dataset with missing values?",
                    "What are some common data visualization techniques and when would you use them?",
                    "Explain statistical concepts like mean, median, mode, and standard deviation.",
                    "Describe a time you used data to tell a story or influence a decision.",
                 ],
                 "Behavioral": [
                    "How do you ensure the accuracy and reliability of your data analysis?",
                    "Describe a situation where you had to present complex data findings to a non-technical audience.",
                    "Tell me about a challenging data analysis project you worked on.",
                    "How do you stay updated with new tools and techniques in data analysis?",
                 ]
             }
        }
    }

    # Select the relevant question pool
    role_questions = all_questions.get(role, all_questions["Software Engineer"]) # Default to SWE
    domain_key = domain if domain != "None" and domain in role_questions else "General"

    # Check if the specific domain exists for the role, otherwise fallback to General for that role
    if domain_key not in role_questions:
        domain_key = "General"

    question_pool = role_questions.get(domain_key, role_questions["General"]) # Fallback to General if domain invalid

    mode_questions = question_pool.get(mode, [])

    # Select random questions
    if len(mode_questions) >= num_questions:
        return random.sample(mode_questions, num_questions)
    elif mode_questions: # Return all if fewer than requested
         return mode_questions
    else: # Fallback if no specific questions found
        return ["Tell me about a project you're proud of and why."]

# --- Answer Evaluation ---
def evaluate_answer(question, answer, role, mode):
    """Evaluates the user's answer using OpenAI_API."""
    if not client:
        return "Evaluation unavailable: OpenAI client not initialized.", None

    if not answer or not answer.strip():
        return "No answer provided. Please try again.", 0 # Score 0 for no answer

    max_retries = 2
    retry_delay = 3 # seconds

    for attempt in range(max_retries + 1):
        try:
            if mode == "Technical":
                system_prompt = f"You are an expert technical interviewer evaluating a candidate for a {role} role. Be concise and constructive."
                user_prompt = f"""Question: {question}
Candidate's Answer: {answer}

Evaluate the technical accuracy, depth, clarity, and relevance of the answer for a {role}.
Provide specific, constructive feedback (2-4 sentences).
Give a score from 1 to 10 based on the quality of the answer.
Format:
Feedback: [Your feedback here]
Score: [Score]/10"""
            else: # Behavioral
                system_prompt = f"You are an expert behavioral interviewer evaluating a candidate for a {role} role using the STAR method (Situation, Task, Action, Result). Be concise and constructive."
                user_prompt = f"""Question: {question}
Candidate's Answer: {answer}

Evaluate the answer based on the STAR method for a {role}. Assess clarity, structure, relevance, and impact.
Provide specific, constructive feedback (2-4 sentences) on how well the STAR method was applied and the overall quality.
Give a score from 1 to 10 based on the quality of the answer.
Format:
Feedback: [Your feedback here]
Score: [Score]/10"""

            response = client.chat.completions.create(
                model="gpt-3.5-turbo", # Or "gpt-4" if preferred and available
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.6, # Balance creativity and consistency
                max_tokens=150,
                n=1,
                stop=None,
            )
            content = response.choices[0].message.content.strip()

            # Basic parsing for feedback and score
            feedback = content
            score = None
            try:
                 # Look for "Score: X/10" pattern
                 score_line = next((line for line in content.split('\n') if 'score:' in line.lower()), None)
                 if score_line:
                     score_str = score_line.split(':')[-1].split('/')[0].strip()
                     score = int(score_str)
                     # Extract feedback part separately if possible
                     feedback_part = content.split("Score:")[0].replace("Feedback:", "").strip()
                     if feedback_part:
                         feedback = feedback_part

            except (ValueError, IndexError, TypeError):
                # If parsing fails, return the raw content as feedback and no score
                feedback = content + " (Score parsing failed)"
                score = None

            return feedback, score

        except openai.RateLimitError as e:
            if attempt < max_retries:
                print(f"Rate limit exceeded. Retrying in {retry_delay}s... ({attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
            else:
                print(f"Rate limit error after multiple retries: {e}")
                return f"Evaluation failed due to rate limits. Please try again later.", None
        except openai.APIConnectionError as e:
             print(f"Network error connecting to OpenAI: {e}")
             return "Evaluation failed due to a network error.", None
        except openai.AuthenticationError:
             print("Authentication error with OpenAI API key.")
             return "Evaluation failed due to API key issue.", None
        except Exception as e:
            print(f"An unexpected error occurred during evaluation: {e}")
            return f"An unexpected error occurred: {e}", None

    return "Evaluation failed after multiple retries.", None


# --- UI Elements ---
role_dropdown = widgets.Dropdown(
    options=["Software Engineer", "Product Manager", "Data Analyst"],
    value="Software Engineer", description="Role:", style={'description_width': 'initial'}
)

domain_dropdown = widgets.Dropdown(
    options=["None", "Frontend", "Backend", "ML", "System Design"],
    value="None", description="Domain:", style={'description_width': 'initial'}
)

mode_radio = widgets.RadioButtons(
    options=["Technical", "Behavioral"], value="Technical", description="Mode:", style={'description_width': 'initial'}
)

start_button = widgets.Button(description="Start Interview", button_style='info')
output_area = widgets.Output()

# --- Interview State ---
interview_state = {
    "questions": [],
    "current_index": 0,
    "results": [],
    "role": "",
    "mode": "",
}

# --- Interview Flow Widgets (will be created dynamically) ---
question_label = widgets.HTML(value="") # Use HTML for better formatting
answer_input = widgets.Textarea(placeholder="Type your answer here...", layout={'width': '95%', 'height': '150px'})
submit_button = widgets.Button(description="Submit Answer", button_style='success', icon='check')
feedback_output = widgets.Output(layout={'border': '1px solid #ccc', 'padding': '10px', 'margin_top': '10px', 'background_color': '#f9f9f9'})
progress_label = widgets.Label(value="")
interview_box = widgets.VBox() # Placeholder for dynamic widgets

# --- Event Handlers ---
def submit_answer_click(b):
    """Handles answer submission, evaluation, and next question."""
    answer = answer_input.value
    index = interview_state['current_index']
    question = interview_state['questions'][index]
    role = interview_state['role']
    mode = interview_state['mode']

    # Disable input/button while processing
    answer_input.disabled = True
    submit_button.disabled = True
    submit_button.description = "Evaluating..."
    submit_button.icon = 'spinner'

    with feedback_output:
        clear_output(wait=True)
        print("⏳ Evaluating your answer...")

    # Evaluate
    feedback_text, score = evaluate_answer(question, answer, role, mode)

    # Store results
    interview_state['results'].append({
        "question": question,
        "answer": answer,
        "feedback": feedback_text,
        "score": score
    })

    # Display feedback
    with feedback_output:
        clear_output(wait=True)
        feedback_html = f"<b>Feedback:</b><br>{feedback_text.replace(chr(10), '<br>')}" # Use <br> for newlines
        if score is not None:
             feedback_html += f"<br><b>Score: {score}/10</b>"
        else:
             feedback_html += "<br><b>Score: N/A</b>"
        display(widgets.HTML(value=feedback_html))


    # Move to next question or summary
    interview_state['current_index'] += 1
    if interview_state['current_index'] < len(interview_state['questions']):
        # Slight delay before showing next question
        time.sleep(1)
        ask_next_question()
        # Re-enable for next question
        answer_input.disabled = False
        submit_button.disabled = False
        submit_button.description = "Submit Answer"
        submit_button.icon = 'check'

    else:
        # End of interview
        submit_button.description = "Finished"
        submit_button.icon = 'stop'
        time.sleep(2) # Pause before showing summary
        display_summary()


def ask_next_question():
    """Displays the next question."""
    index = interview_state['current_index']
    total_questions = len(interview_state['questions'])
    question = interview_state['questions'][index]

    progress_label.value = f"Question {index + 1} of {total_questions}"
    question_label.value = f"<b>Question {index + 1}:</b><br>{question}" # Bold question number
    answer_input.value = "" # Clear previous answer
    with feedback_output:
        clear_output() # Clear previous feedback

def start_interview_click(b):
    """Starts the interview process."""
    if not client and OPENAI_API_KEY != "YOUR_OPENAI_API_KEY": # Only show warning if key was expected
         with output_area:
            clear_output(wait=True)
            print("⚠️ Cannot start evaluation: OpenAI client not initialized.")
            print("   Check your API key and restart the runtime if you want evaluation.")
            print("   Starting interview without evaluation...")
            time.sleep(3) # Pause to let user read
            # Continue without evaluation if key is missing/invalid
    elif not client:
         with output_area:
            clear_output(wait=True)
            print("ℹ️ Starting interview without evaluation (No API Key provided).")
            time.sleep(2)

    # Get selections
    role = role_dropdown.value
    domain = domain_dropdown.value
    mode = mode_radio.value

    # Generate questions
    questions = generate_questions(role, domain, mode)
    if not questions:
         with output_area:
            clear_output(wait=True)
            print(f"❌ Could not find/generate questions for {role} - {domain} - {mode}.")
            print("   Please try different selections.")
         return

    # Reset state
    interview_state["questions"] = questions
    interview_state["current_index"] = 0
    interview_state["results"] = []
    interview_state["role"] = role
    interview_state["mode"] = mode

    # Update UI
    submit_button.on_click(submit_answer_click) # Attach handler
    submit_button.disabled = False
    submit_button.description = "Submit Answer"
    submit_button.icon = 'check'

    # Assemble the interview UI elements
    global interview_box # Need to modify the global VBox
    interview_box.children = [
        progress_label,
        question_label,
        answer_input,
        submit_button,
        feedback_output
    ]

    # Display the interview UI
    with output_area:
        clear_output(wait=True)
        print(f"🚀 Starting {mode} interview for {role} ({domain if domain != 'None' else 'General'})...")
        display(interview_box)

    # Ask the first question
    ask_next_question()

# --- Initial Display ---
start_button.on_click(start_interview_click)

print("--- Configure Your Mock Interview ---")
# Display initial setup widgets
setup_box = widgets.VBox([role_dropdown, domain_dropdown, mode_radio, start_button])
display(setup_box)
display(output_area) # Area where interview or messages will appear
